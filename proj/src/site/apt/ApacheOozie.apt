													-----
													Title: maven site documentation (Apache Oozie) 
													-----
													Author:RavikiranGoru,ramachandran289... 
													-----
													Date: 2017-09-05
													-----

 <<Apache Oozie:>> it is a tool or framework. Apache Oozie is a scheduler system to run and manage Hadoop jobs in a distributed environment.
		it allows user to execute jobs(Hive, Pig, sqoop, Java, drill, shell etc) in desired order and/or in scheduled manner in Hadoop's distributed env.
		It is responsible for triggering the workflow actions, which in turn uses the Hadoop execution engine to actually execute the task.
		
		Oozie detects completion of tasks through callback and polling. When Oozie starts a task, it provides a unique callback HTTP URL to the task, and notifies that URL when it is complete. 
		If the task fails to invoke the callback URL, Oozie can poll the task for completion.

 Three types of jobs are common in Oozie:

	[[1]] Oozie Workflow Jobs:
		These are represented as Directed Acyclic Graphs (DAGs) to specify a sequence of actions to be executed.
		
	[[2]] Oozie Coordinator Jobs:
		These consist of workflow jobs triggered by time and data availability.
		
	[[3]] Oozie Bundle:
		These can be referred to as a package of multiple coordinator and workflow jobs.
	
	[]	
		
 <<Oozie editor:>>  {{http://gethue.com/new-apache-oozie-workflow-coordinator-bundle-editors/}}

 <<WorkFlow in oozie:>>	Workflow in Oozie is a sequence of actions arranged in a control dependency DAG (Direct Acyclic Graph).
	The actions are in controlled dependency as the next action can only run as per the output of current action.
	Subsequent actions are dependent on its previous action. 
	
	A workflow action can be a Hive action, Pig action, Java action, Shell action, etc. There can be decision trees to decide how and on which condition a job should run.
	
	A fork is used to run multiple jobs in parallel. Oozie workflows can be parameterized (variables like ${nameNode} can be passed within the workflow definition). 
	These parameters come from a configuration file called as property file.
			
	Hadoop Developers use Oozie for performing ETL operations on data in a sequential order and saving the output in a specified format (Avro, ORC, etc.) in HDFS.		
		
 <In an enterprise, Oozie jobs are scheduled as coordinators or bundles.>

 {{https://www.tutorialspoint.com/apache_oozie/apache_oozie_workflow.htm}}



+-------------------------------+
<!-- This is a comment -->
<workflow-app xmlns = "uri:oozie:workflow:0.4" name = "simple-Workflow">
  
   <start to = "Create_External_Table" />
   <!—Step 1 -->
   <action name = "Create_External_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/external.hive</script>
      </hive>
		
      <ok to = "Create_orc_Table" />
      <error to = "kill_job" />
   </action>

   <!—Step 2 -->
   <action name = "Create_orc_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/orc.hive</script>
      </hive>
		
      <ok to = "Insert_into_Table" />
      <error to = "kill_job" />
   </action>

   <!—Step 3 -->
   <action name = "Insert_into_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/Copydata.hive</script>
         <param>database_name</param>
      </hive>
		
      <ok to = "end" />
      <error to = "kill_job" />
   </action>
   
   <kill name = "kill_job">
      <message>Job failed</message>
   </kill>
	
   <end name = "end" />

</workflow-app>
+-------------------------------+

 <<Fork and Join Control Node in Workflow (Paraller tasks)>>

+-------------------------------+
<workflow-app xmlns = "uri:oozie:workflow:0.4" name = "simple-Workflow">
   <start to = "fork_node" />
   
   <fork name = "fork_node">
      <path start = "Create_External_Table"/>
      <path start = "Create_orc_Table"/>
   </fork>
   
   <action name = "Create_External_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/external.hive</script>
      </hive>
      
      <ok to = "join_node" />
      <error to = "kill_job" />
   </action>
   
   <action name = "Create_orc_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/orc.hive</script>
      </hive>
		
      <ok to = "join_node" />
      <error to = "kill_job" />
   </action>
   
   <join name = "join_node" to = "Insert_into_Table"/>
	
   <action name = "Insert_into_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/Copydata.hive</script>
         <param>database_name</param>
      </hive>
		
      <ok to = "end" />
      <error to = "kill_job" />
   </action>
   
   <kill name = "kill_job">
      <message>Job failed</message>
   </kill>
   
   <end name = "end" />
	
</workflow-app>
+-------------------------------+

 <<Decision Nodes in Workflow>>

+-------------------------------+
<workflow-app xmlns = "uri:oozie:workflow:0.4" name = "simple-Workflow">
   <start to = "external_table_exists" />
   
   <decision name = "external_table_exists">
      <switch>
         <case to = "Create_External_Table">${fs:exists('/test/abc') eq 'false'}
            </case>
         <default to = "orc_table_exists" />
      </switch>
   </decision>

   <action name = "Create_External_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/external.hive</script>
      </hive>
		
      <ok to = "orc_table_exists" />
      <error to = "kill_job" />
   </action>
   
   <decision name = "orc_table_exists">
      <switch>
         <case to = "Create_orc_Table">
            ${fs:exists('/apps/hive/warehouse/orc_table') eq 'false'}</case>
         <default to = "Insert_into_Table" />
      </switch>
   </decision>
   
   <action name = "Create_orc_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/orc.hive</script>
      </hive>
		
      <ok to = "Insert_into_Table" />
      <error to = "kill_job" />
   </action>
   
   <action name = "Insert_into_Table">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>xyz.com:8088</job-tracker>
         <name-node>hdfs://rootname</name-node>
         <script>hdfs_path_of_script/Copydata.hive</script>
         <param>database_name</param>
      </hive>
		
      <ok to = "end" />
      <error to = "kill_job" />
   </action>
   
   <kill name = "kill_job">
      <message>Job failed</message>
   </kill>
	
   <end name = "end" />
	
</workflow-app>
+-------------------------------+

 <Coordinator applications allow users to schedule complex workflows, including workflows that are scheduled regularly.> 
 
 <Oozie Coordinator models the workflow execution triggers in the form of time, data or event predicates.>



